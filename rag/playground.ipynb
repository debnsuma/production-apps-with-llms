{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a RAG application from scratch using `LangChain` and `Bedrock`\n",
    "\n",
    "Before we get started, I would like to thank to [Santiago](https://www.youtube.com/@underfitted) for his wonderful tutorial on `LangChain`, this notebook is based on his tutorial. \n",
    "Here is a high-level overview of the system we want to build:\n",
    "\n",
    "<img src='./images/IMG_0354.jpg' width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `.env `file in the project directory using env.example as a reference. Populate the .env file with your Aurora PostgreSQL DB cluster details (this we will need later on):\n",
    "\n",
    "```\n",
    "PGVECTOR_DRIVER='psycopg2'\n",
    "PGVECTOR_USER='<<Username>>'\n",
    "PGVECTOR_PASSWORD='<<Password>>'\n",
    "PGVECTOR_HOST='<<Aurora DB cluster host>>'\n",
    "PGVECTOR_PORT=5432\n",
    "PGVECTOR_DATABASE='<<DBName>>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the `env` variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the environment variables we need to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint as pp\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to access any LLM/FM from `Bedrock` ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control plane \n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-west-2', \n",
    ")\n",
    "\n",
    "#Create the connection to Bedrock\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-west-2', \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon.titan-tg1-large',\n",
      " 'amazon.titan-embed-g1-text-02',\n",
      " 'amazon.titan-text-lite-v1:0:4k',\n",
      " 'amazon.titan-text-lite-v1',\n",
      " 'amazon.titan-text-express-v1:0:8k',\n",
      " 'amazon.titan-text-express-v1',\n",
      " 'amazon.titan-text-agile-v1',\n",
      " 'amazon.titan-embed-text-v1:2:8k',\n",
      " 'amazon.titan-embed-text-v1',\n",
      " 'amazon.titan-embed-text-v2:0:8k',\n",
      " 'amazon.titan-embed-text-v2:0',\n",
      " 'amazon.titan-embed-image-v1:0',\n",
      " 'amazon.titan-embed-image-v1',\n",
      " 'amazon.titan-image-generator-v1:0',\n",
      " 'amazon.titan-image-generator-v1',\n",
      " 'stability.stable-diffusion-xl-v1:0',\n",
      " 'stability.stable-diffusion-xl-v1',\n",
      " 'ai21.j2-grande-instruct',\n",
      " 'ai21.j2-jumbo-instruct',\n",
      " 'ai21.j2-mid',\n",
      " 'ai21.j2-mid-v1',\n",
      " 'ai21.j2-ultra',\n",
      " 'ai21.j2-ultra-v1:0:8k',\n",
      " 'ai21.j2-ultra-v1',\n",
      " 'anthropic.claude-instant-v1:2:100k',\n",
      " 'anthropic.claude-instant-v1',\n",
      " 'anthropic.claude-v2:0:18k',\n",
      " 'anthropic.claude-v2:0:100k',\n",
      " 'anthropic.claude-v2:1:18k',\n",
      " 'anthropic.claude-v2:1:200k',\n",
      " 'anthropic.claude-v2:1',\n",
      " 'anthropic.claude-v2',\n",
      " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
      " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
      " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
      " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
      " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
      " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
      " 'anthropic.claude-3-opus-20240229-v1:0',\n",
      " 'cohere.command-text-v14:7:4k',\n",
      " 'cohere.command-text-v14',\n",
      " 'cohere.command-r-v1:0',\n",
      " 'cohere.command-r-plus-v1:0',\n",
      " 'cohere.command-light-text-v14:7:4k',\n",
      " 'cohere.command-light-text-v14',\n",
      " 'cohere.embed-english-v3:0:512',\n",
      " 'cohere.embed-english-v3',\n",
      " 'cohere.embed-multilingual-v3:0:512',\n",
      " 'cohere.embed-multilingual-v3',\n",
      " 'meta.llama2-13b-chat-v1:0:4k',\n",
      " 'meta.llama2-13b-chat-v1',\n",
      " 'meta.llama2-70b-chat-v1:0:4k',\n",
      " 'meta.llama2-70b-chat-v1',\n",
      " 'meta.llama2-13b-v1:0:4k',\n",
      " 'meta.llama2-13b-v1',\n",
      " 'meta.llama2-70b-v1:0:4k',\n",
      " 'meta.llama2-70b-v1',\n",
      " 'meta.llama3-8b-instruct-v1:0',\n",
      " 'meta.llama3-70b-instruct-v1:0',\n",
      " 'mistral.mistral-7b-instruct-v0:2',\n",
      " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
      " 'mistral.mistral-large-2402-v1:0']\n"
     ]
    }
   ],
   "source": [
    "presently_supported_models = [model['modelId'] for model in bedrock.list_foundation_models()['modelSummaries']]\n",
    "pp(presently_supported_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Explain batch normalization and why its useful?\"\"\"\n",
    "\n",
    "body = {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 1024,\n",
    "            \"messages\": [\n",
    "                 {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt_data\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "body = json.dumps(body) # Encode body as JSON string\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "accept = 'application/json'\n",
    "contentType = 'application/json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch normalization is a technique used in deep neural networks to improve the stability and performance of the training process. It was introduced by Sergey Ioffe and Christian Szegedy in 2015. Here's an explanation of batch normalization and why it's useful:\n",
      "\n",
      "1. **Internal Covariate Shift**: During the training of deep neural networks, the distribution of the inputs to a layer can change as the parameters of the previous layers are updated. This phenomenon is known as \"internal covariate shift.\" It can slow down the training process and make it difficult for the network to converge.\n",
      "\n",
      "2. **Normalization of Inputs**: Batch normalization addresses this issue by normalizing the inputs to each layer. It does this by subtracting the batch mean and dividing by the batch standard deviation. This ensures that the inputs to each layer have a mean of zero and a standard deviation of one.\n",
      "\n",
      "3. **Improved Gradient Flow**: By normalizing the inputs, batch normalization allows for better gradient flow during the backpropagation process. This means that the gradients can flow more easily through the network, making it easier to update the weights and biases.\n",
      "\n",
      "4. **Reduced Dependence on Weight Initialization**: Batch normalization also reduces the dependence of the network on the initialization of the weights. This is because the normalization of the inputs makes the network less sensitive to the initial values of the weights.\n",
      "\n",
      "5. **Regularization Effect**: Batch normalization has a regularization effect, which helps to prevent overfitting. This is because the normalization of the inputs introduces a form of noise that acts as a regularizer.\n",
      "\n",
      "6. **Higher Learning Rates**: Because of the improved gradient flow and reduced dependence on weight initialization, batch normalization allows for the use of higher learning rates during training. This can lead to faster convergence and better performance.\n",
      "\n",
      "In summary, batch normalization is useful because it helps to stabilize the training process of deep neural networks, reduces the dependence on weight initialization, provides a regularization effect, and allows for the use of higher learning rates. By addressing the issue of internal covariate shift and normalizing the inputs to each layer, batch normalization improves the overall performance and convergence of deep neural networks.\n"
     ]
    }
   ],
   "source": [
    "response = bedrock_runtime.invoke_model(body=body,\n",
    "                                        modelId=model_id, \n",
    "                                        accept=accept, \n",
    "                                        contentType=contentType)\n",
    "\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "for output in response_body.get(\"content\", []):\n",
    "    print(output[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `LangChain`\n",
    "Let's define the LLM model that we'll use as part of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "model = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\", model_kwargs={\"temperature\": 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the model by asking a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The 2019 ICC Cricket World Cup was won by England. It was hosted in England and Wales.\\n\\nIn the final at Lord's Cricket Ground in London, England defeated New Zealand in a dramatic match that went to a Super Over tie-breaker after the scores were tied at the end of the regulation 50 overs per side.\\n\\nEngland scored 241/8 in their 50 overs, which New Zealand also scored to tie the match. In the Super Over, both teams scored 15 runs each. However, England was awarded the World Cup title on the basis of having scored more boundaries (fours and sixes) during the match.\\n\\nIt was England's first ever Cricket World Cup title victory. New Zealand finished as the runners-up for the second consecutive World Cup after 2015.\", additional_kwargs={'usage': {'prompt_tokens': 20, 'completion_tokens': 174, 'total_tokens': 194}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, response_metadata={'usage': {'prompt_tokens': 20, 'completion_tokens': 174, 'total_tokens': 194}, 'stop_reason': 'end_turn', 'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}, id='run-210c87a3-a65d-4c6d-8b63-3721362d68ff-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_data = \"\"\"Who won the ICC Criket World Cup 2019?\"\"\"\n",
    "model.invoke(prompt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2019 ICC Cricket World Cup was won by England. It was hosted in England and Wales.\n",
      "\n",
      "In the final at Lord's Cricket Ground in London, England defeated New Zealand in a dramatic match that went to a Super Over tie-breaker after the scores were tied at the end of the regulation 50 overs per side.\n",
      "\n",
      "England scored 241/8 in their 50 overs, which New Zealand also scored to tie the match. In the Super Over, both teams scored 15 runs each. However, England was awarded the World Cup title on the basis of having scored more boundaries (fours and sixes) during the match.\n",
      "\n",
      "It was England's first ever Cricket World Cup title victory. New Zealand finished as the runners-up for the second consecutive World Cup after 2015.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "\n",
    "print(chain.invoke(\"Who won the ICC Criket World Cup 2019?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribing the YouTube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context we want to send the model comes from a YouTube video. Let's download the video and transcribe it using Amazon Transcribe\n",
    "\n",
    "<img src='./images/IMG_0354.jpg' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription file already exists.\n"
     ]
    }
   ],
   "source": [
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=lB_0hR5s41Y&ab_channel=BeerBiceps\"\n",
    "S3_BUCKET = 'ml-dl-demo-data'\n",
    "\n",
    "from utils import transcribe_video\n",
    "\n",
    "transcribe_video(s3_bucket_name=S3_BUCKET, youtube_video_url=YOUTUBE_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the transcription and display the first few characters to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're a multibillionaire European founder who's moved to Gandhinagar. Yes. Why did you choose Gujar\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    transcription = json.loads(file.read())\n",
    "    transcription = transcription['results']['transcripts'][0]['transcript']\n",
    "\n",
    "transcription[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114926"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Fabian, the most important factor when selecting a location for a business in India is not the location itself, but finding the right person/leader to head the operations there. He says:\n",
      "\n",
      "\"The key to succeed a new company is not the location, the key is the director. This is the most important thing. You can have a good director in a bad location, it will work. You can have a good location, average director, it's gonna be painful.\"\n",
      "\n",
      "He explains that when expanding to a new city/region in India, they first identify a strong leader/director from within the company who can lead the new operations. The location is secondary - they go wherever that capable person is based or willing to relocate to. \n",
      "\n",
      "For example, he chose Gandhinagar not because of the city itself, but because the director he wanted to work with happened to be based there initially. The focus is on finding the right leadership talent first, and then establishing the operations in whatever location makes sense for that person.\n"
     ]
    }
   ],
   "source": [
    "# Sending the entire transcript --> SLOW <not recommended>\n",
    "response = chain.invoke({\"context\": transcription,\n",
    "                         \"question\": \"What matters when selecting a location for a business in India ?\"\n",
    "                        })\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the transcription\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we can't use the entire transcription as the context for the model, a potential solution is to split the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question:\n",
    "\n",
    "<img src='./images/IMG_0358.jpg' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load using TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "\n",
    "# 2. Split using RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the relevant chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of **embeddings** comes into play.\n",
    "\n",
    "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document. It's a projection of a concept in a high-dimensional space. Embeddings have a simple characteristic: The projection of related concepts will be close to each other, while concepts with different meanings will lie far away. \n",
    "\n",
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model:\n",
    "\n",
    "<img src='./images/IMG_0359.jpg' width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an efficient way to store document chunks, their embeddings, and perform similarity searches at scale. To do this, we'll use a **vector store**.\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches. \n",
    "\n",
    "<img src='./images/IMG_0360.jpg' width=\"1200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "import os\n",
    "\n",
    "# Initialize the embeddings\n",
    "embeddings = BedrockEmbeddings()\n",
    "\n",
    "# Set the collection name\n",
    "COLLECTION_NAME = \"rag-intro-yt\"\n",
    "\n",
    "# Connection String\n",
    "CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "                                                                driver=os.getenv(\"PGVECTOR_DRIVER\"),\n",
    "                                                                user=os.getenv(\"PGVECTOR_USER\"),\n",
    "                                                                password=os.getenv(\"PGVECTOR_PASSWORD\"),\n",
    "                                                                host=os.getenv(\"PGVECTOR_HOST\"),\n",
    "                                                                port=os.getenv(\"PGVECTOR_PORT\"),\n",
    "                                                                database=os.getenv(\"PGVECTOR_DATABASE\"),\n",
    "                                                            )\n",
    "\n",
    "# Create the PGVector instance from the documents\n",
    "db = PGVector.from_documents(\n",
    "                                embedding=embeddings,\n",
    "                                documents=documents,\n",
    "                                collection_name=COLLECTION_NAME,\n",
    "                                connection_string=CONNECTION_STRING,\n",
    "                                use_jsonb = True\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on Aurora to make sure everything works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='to and IIII I love business. So I was reading the 223 books, uh, about management psychology to develop my business every week. Um, I did a lot of things and this one which is to do work better than the others. And so I started alone after two years, I had one employee after three years, 34 or five, this is in 2004. That was in 2002. Yes, when I was still a student and then I grew, uh organically bootstrapped the company. Uh I started to sell services on this software. So the software is, is a management software. At the beginning, I was selling services. So I asked Ken, what do you want? And I was developing everything and I did that until 2 2010. And at that time, I had a software that had a lot of features because I developed everything the customer would asked for. So it was full of feature but ugly complex uh and everything. Uh And at that time, I had uh 100 employees. Um So I decided to do a pivot in the business me and say we cannot do service and develop everything people', metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"using every day. You should check it and maybe one of them will pay. So based on those 10% parts of your software that are actually premium which people pay for. You're supporting the whole business, including the 90% which is for me, it was a very philosophical problem because I was passionate about about source and I did not want to sell proper source of software. I was against that. Uh But it was a very bad decision. It was a very bright mindset because at the time I had something like maybe 100 developers maximum, I couldn't pay more. Um And now that I have 10% non open source or 20% non open source, 80% open source, I can afford paying hundreds of developers. Uh So I contribute way more to the open source world than what I did before, even though I'm not 100% open source today. So it's something I didn't understand, took me years because I refused that. But once I did that, I switched the business model and then the company started to skyrocket. You know, there's a lot of lessons\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"per year. So it's a massive market 1/5 of the world population. So you cannot close your eye to such a big and future economy. So I came here in order to develop the local market, you know, in order to understand you better, why don't you introduce your own life's work a little bit before I ask you more questions about India and Europe. OK. Probably the right way to start because people especially business oriented audiences, which is a big chunk of youtube audiences now are very interested to know about how the world looks at India and about how you want to sell in India. And why do you want to sell here? Because honestly, Indians, if you look at it from a mass scale, are not easy customers, they're difficult customers to win over. So I want to get into all those tangents about all your challenges you faced here, et cetera, et cetera. But let's begin with a little bit about your story. Yes, you began as a coder. Yeah, I was a developer uh engineer I started to and IIII I love\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Can you share the detail of the speaker's journey from starting as a coder to becoming a successful entrepreneur, including the pivot in his business model?\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
    "\n",
    "<img src='./images/IMG_0361.jpg' width=\"1200\">\n",
    "\n",
    "We need to configure a [Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/). The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store we created before: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Aurora as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, here are the main challenges and advantages of doing business in India mentioned:\n",
      "\n",
      "Advantages:\n",
      "1. India is an open market and easy to do business in, even easier than the US according to the speaker.\n",
      "2. Indian customers are willing to use products that may not be perfect, unlike Americans who are very picky.\n",
      "3. The decision-making process is faster in India compared to Europe or Africa.\n",
      "4. India has a large pool of developers that can provide services at an affordable cost.\n",
      "\n",
      "Challenges:\n",
      "1. The Indian market is very price-sensitive, and customers prioritize affordability over quality.\n",
      "2. Indian customers often think they can do everything themselves and are reluctant to buy services, preferring to just buy the software.\n",
      "3. Adapting products and services to meet Indian market needs, such as pricing in rupees, complying with GST and other regulations.\n",
      "4. Building brand awareness and marketing reach, as the speaker's company was initially known only among developers and not businessmen.\n",
      "\n",
      "Overall, the context highlights India's cost-conscious market, the need for localization and competitive pricing, but also the advantages of a large customer base, faster decision-making, and access to affordable development resources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": db.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "response = chain.invoke(\"What are the main challenges and advantages of doing business in India, including insights on market sensitivity, price, and speed of decision-making?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Building a RAG application from scratch using Python - By Santiago](https://www.youtube.com/watch?v=BrsocJb-fAo&t=548s&ab_channel=Underfitted)\n",
    "- [Vector Embeddings and RAG Demystified: Leveraging Amazon Bedrock, Aurora, and LangChain - Part 1](https://community.aws/content/2gvh6fQM4mJQduLye3mHlCNvPxX/vector-embeddings-and-rag-demystified)\n",
    "- [Vector Embeddings and RAG Demystified: Leveraging Amazon Bedrock, Aurora, and LangChain - Part 2](https://community.aws/content/2gvh8oJzNrM4vxdZDd903zcEFJc/vector-embeddings-and-rag-demystified-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“genai”",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
